{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-22T08:19:46.118634Z","iopub.execute_input":"2024-02-22T08:19:46.118975Z","iopub.status.idle":"2024-02-22T08:19:47.196742Z","shell.execute_reply.started":"2024-02-22T08:19:46.118946Z","shell.execute_reply":"2024-02-22T08:19:47.195777Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/transformer/__results__.html\n/kaggle/input/transformer/__notebook__.ipynb\n/kaggle/input/transformer/__output__.json\n/kaggle/input/transformer/model.pt\n/kaggle/input/transformer/custom.css\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install datasets --quiet\n!pip install evaluate --quiet","metadata":{"execution":{"iopub.status.busy":"2024-02-22T13:08:17.861336Z","iopub.execute_input":"2024-02-22T13:08:17.861999Z","iopub.status.idle":"2024-02-22T13:08:44.970782Z","shell.execute_reply.started":"2024-02-22T13:08:17.861965Z","shell.execute_reply":"2024-02-22T13:08:44.969311Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nimport math\nimport copy\n\nfilepath = '/kaggle/working/'\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-02-22T08:19:59.654544Z","iopub.execute_input":"2024-02-22T08:19:59.655050Z","iopub.status.idle":"2024-02-22T08:20:03.668235Z","shell.execute_reply.started":"2024-02-22T08:19:59.655019Z","shell.execute_reply":"2024-02-22T08:20:03.667222Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"from datasets import load_dataset\n\ndata = load_dataset(\n    \"mt_eng_vietnamese\",\n    \"iwslt2015-en-vi\"\n)\ndata","metadata":{"execution":{"iopub.status.busy":"2024-02-22T08:20:15.927467Z","iopub.execute_input":"2024-02-22T08:20:15.927977Z","iopub.status.idle":"2024-02-22T08:20:30.408677Z","shell.execute_reply.started":"2024-02-22T08:20:15.927946Z","shell.execute_reply":"2024-02-22T08:20:30.407773Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.88k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"630d66c743654f59a031d0190980722b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/1.08k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efab54704f2f4dffb2127a960eddcd65"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset mt_eng_vietnamese/iwslt2015-en-vi (download: 30.83 MiB, generated: 31.59 MiB, post-processed: Unknown size, total: 62.42 MiB) to /root/.cache/huggingface/datasets/mt_eng_vietnamese/iwslt2015-en-vi/1.0.0/53add551a01e9874588066f89d42925f9fad43db347199dad00f7e4b0c905a71...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/13.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3865727b7e7543f8a5fd88bdbaf00fc7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/18.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7f74f142580416792031f79f80eeacd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/140k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b267a31539d4cf58cd9e724587353de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/188k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56af4a3346bd4eb4906dcc82a603183d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/132k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27540045bcc74cb6b61972119a71241c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/184k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e727f176ea0e40e399506ad9ede7489f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/133318 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1269 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1269 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset mt_eng_vietnamese downloaded and prepared to /root/.cache/huggingface/datasets/mt_eng_vietnamese/iwslt2015-en-vi/1.0.0/53add551a01e9874588066f89d42925f9fad43db347199dad00f7e4b0c905a71. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f90e6f73f45e45dea19f7d2d8f888a00"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['translation'],\n        num_rows: 133318\n    })\n    validation: Dataset({\n        features: ['translation'],\n        num_rows: 1269\n    })\n    test: Dataset({\n        features: ['translation'],\n        num_rows: 1269\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"train_data = data['train']\ntest_data = data['test']\nvalid_data = data['validation']","metadata":{"execution":{"iopub.status.busy":"2024-02-22T08:20:32.496941Z","iopub.execute_input":"2024-02-22T08:20:32.497509Z","iopub.status.idle":"2024-02-22T08:20:32.502079Z","shell.execute_reply.started":"2024-02-22T08:20:32.497479Z","shell.execute_reply":"2024-02-22T08:20:32.501091Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_data[3]","metadata":{"execution":{"iopub.status.busy":"2024-02-22T08:20:35.366017Z","iopub.execute_input":"2024-02-22T08:20:35.366406Z","iopub.status.idle":"2024-02-22T08:20:35.373847Z","shell.execute_reply.started":"2024-02-22T08:20:35.366376Z","shell.execute_reply":"2024-02-22T08:20:35.372800Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'translation': {'en': 'Headlines that look like this when they have to do with climate change , and headlines that look like this when they have to do with air quality or smog .',\n  'vi': 'Có những dòng trông như thế này khi bàn về biến đổi khí hậu , và như thế này khi nói về chất lượng không khí hay khói bụi .'}}"},"metadata":{}}]},{"cell_type":"code","source":"SOURCE_LANG = 'en'\nTARGET_LANG = 'vi'\n\nUNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\nSPECIAL_SYMBOLS = ['<unk>', '<pad>', '<sos>', '<eos>']","metadata":{"execution":{"iopub.status.busy":"2024-02-22T08:20:36.006107Z","iopub.execute_input":"2024-02-22T08:20:36.007208Z","iopub.status.idle":"2024-02-22T08:20:36.011987Z","shell.execute_reply.started":"2024-02-22T08:20:36.007171Z","shell.execute_reply":"2024-02-22T08:20:36.010998Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\n\ntoken_transform = {}\nvocab = {}\ntoken_transform[SOURCE_LANG] = get_tokenizer('basic_english')\ntoken_transform[TARGET_LANG] = get_tokenizer('basic_english')","metadata":{"execution":{"iopub.status.busy":"2024-02-22T08:20:36.701442Z","iopub.execute_input":"2024-02-22T08:20:36.701840Z","iopub.status.idle":"2024-02-22T08:20:36.943138Z","shell.execute_reply.started":"2024-02-22T08:20:36.701809Z","shell.execute_reply":"2024-02-22T08:20:36.942277Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"token_transform['en'](\"hello it's me\")","metadata":{"execution":{"iopub.status.busy":"2024-02-22T08:20:37.551690Z","iopub.execute_input":"2024-02-22T08:20:37.552062Z","iopub.status.idle":"2024-02-22T08:20:37.558359Z","shell.execute_reply.started":"2024-02-22T08:20:37.552034Z","shell.execute_reply":"2024-02-22T08:20:37.557437Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"['hello', 'it', \"'\", 's', 'me']"},"metadata":{}}]},{"cell_type":"code","source":"def tokenize_example(example, sos_token, eos_token, token_transform, src_lang, tgt_lang):\n    en_tokens = token_transform['en'](example['translation']['en'])\n    vi_tokens = token_transform['vi'](example['translation']['vi'])\n\n    en_tokens = ([sos_token] + en_tokens + [eos_token])\n    vi_tokens = ([sos_token] + vi_tokens + [eos_token])\n\n    return {\"en_tokens\": (en_tokens), \"vi_tokens\":(vi_tokens)}","metadata":{"execution":{"iopub.status.busy":"2024-02-22T08:20:38.495597Z","iopub.execute_input":"2024-02-22T08:20:38.495942Z","iopub.status.idle":"2024-02-22T08:20:38.501700Z","shell.execute_reply.started":"2024-02-22T08:20:38.495904Z","shell.execute_reply":"2024-02-22T08:20:38.500685Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"fn_kwargs = {\n    'sos_token': '<sos>',\n    'eos_token': '<eos>',\n    'token_transform': token_transform,\n    'src_lang': SOURCE_LANG,\n    'tgt_lang': TARGET_LANG,\n    }\ntrain_data = train_data.map(tokenize_example, fn_kwargs = fn_kwargs)\ntest_data = test_data.map(tokenize_example, fn_kwargs = fn_kwargs)\nvalid_data = valid_data.map(tokenize_example, fn_kwargs = fn_kwargs)","metadata":{"execution":{"iopub.status.busy":"2024-02-22T08:20:39.452389Z","iopub.execute_input":"2024-02-22T08:20:39.453251Z","iopub.status.idle":"2024-02-22T08:20:58.634262Z","shell.execute_reply.started":"2024-02-22T08:20:39.453217Z","shell.execute_reply":"2024-02-22T08:20:58.633480Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/133318 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d1c67ca02d84e68a12f094c3e00b98b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1269 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cc4711cd4894469b3d8a7e774fb063b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1269 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38a903c3967f42ed924793a725c6eefd"}},"metadata":{}}]},{"cell_type":"code","source":"print(train_data[0])","metadata":{"execution":{"iopub.status.busy":"2024-02-22T08:21:02.164470Z","iopub.execute_input":"2024-02-22T08:21:02.165331Z","iopub.status.idle":"2024-02-22T08:21:02.172063Z","shell.execute_reply.started":"2024-02-22T08:21:02.165297Z","shell.execute_reply":"2024-02-22T08:21:02.170992Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"{'translation': {'en': 'Rachel Pike : The science behind a climate headline', 'vi': 'Khoa học đằng sau một tiêu đề về khí hậu'}, 'en_tokens': ['<sos>', 'rachel', 'pike', 'the', 'science', 'behind', 'a', 'climate', 'headline', '<eos>'], 'vi_tokens': ['<sos>', 'khoa', 'học', 'đằng', 'sau', 'một', 'tiêu', 'đề', 'về', 'khí', 'hậu', '<eos>']}\n","output_type":"stream"}]},{"cell_type":"code","source":"for lang in [SOURCE_LANG, TARGET_LANG]:\n    vocab[lang] = build_vocab_from_iterator(\n        train_data[lang + '_tokens'],\n        min_freq = 1,\n        specials = SPECIAL_SYMBOLS,\n        special_first = True\n    )\n    vocab[lang].set_default_index(UNK_IDX)","metadata":{"execution":{"iopub.status.busy":"2024-02-22T08:21:03.607796Z","iopub.execute_input":"2024-02-22T08:21:03.608472Z","iopub.status.idle":"2024-02-22T08:21:16.257478Z","shell.execute_reply.started":"2024-02-22T08:21:03.608441Z","shell.execute_reply":"2024-02-22T08:21:16.256503Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"print(vocab['vi'].get_itos()[:10])\nprint(len(vocab['vi']))\nprint(vocab['en'].get_itos()[:10])\nprint(len(vocab['en']))","metadata":{"execution":{"iopub.status.busy":"2024-02-22T08:21:17.562829Z","iopub.execute_input":"2024-02-22T08:21:17.563668Z","iopub.status.idle":"2024-02-22T08:21:17.575530Z","shell.execute_reply.started":"2024-02-22T08:21:17.563632Z","shell.execute_reply":"2024-02-22T08:21:17.574641Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"['<unk>', '<pad>', '<sos>', '<eos>', ',', '.', 'và', 'tôi', 'là', 'một']\n21114\n['<unk>', '<pad>', '<sos>', '<eos>', ',', '.', 'the', 'and', 'to', '&apos']\n47271\n","output_type":"stream"}]},{"cell_type":"code","source":"def numericalize_example(example, vocab, src_lang, tgt_lang):\n    en_ids = torch.tensor(vocab[src_lang].lookup_indices(example['en_tokens']))\n    vi_ids = torch.tensor(vocab[tgt_lang].lookup_indices(example['vi_tokens']))\n\n    return {'en_ids': en_ids, 'vi_ids': vi_ids}","metadata":{"execution":{"iopub.status.busy":"2024-02-22T08:21:19.248319Z","iopub.execute_input":"2024-02-22T08:21:19.249041Z","iopub.status.idle":"2024-02-22T08:21:19.254047Z","shell.execute_reply.started":"2024-02-22T08:21:19.249010Z","shell.execute_reply":"2024-02-22T08:21:19.253100Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"fn_kwargs = {\n    'vocab': vocab,\n    'src_lang': SOURCE_LANG,\n    'tgt_lang': TARGET_LANG,\n    }\ntrain_data = train_data.map(numericalize_example, fn_kwargs = fn_kwargs)\ntest_data = test_data.map(numericalize_example, fn_kwargs = fn_kwargs)\nvalid_data = valid_data.map(numericalize_example, fn_kwargs = fn_kwargs)","metadata":{"execution":{"iopub.status.busy":"2024-02-22T08:21:20.254575Z","iopub.execute_input":"2024-02-22T08:21:20.255267Z","iopub.status.idle":"2024-02-22T08:21:58.737677Z","shell.execute_reply.started":"2024-02-22T08:21:20.255236Z","shell.execute_reply":"2024-02-22T08:21:58.736800Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/133318 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cd535e245b54533bd40ca131f9a7c6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1269 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2004fd93a1f49d690d656b28e6d2518"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1269 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8754e6039e534b638d2b035fe67f45b0"}},"metadata":{}}]},{"cell_type":"code","source":"train_data[0]","metadata":{"execution":{"iopub.status.busy":"2024-02-22T08:22:08.130415Z","iopub.execute_input":"2024-02-22T08:22:08.130781Z","iopub.status.idle":"2024-02-22T08:22:08.139095Z","shell.execute_reply.started":"2024-02-22T08:22:08.130751Z","shell.execute_reply":"2024-02-22T08:22:08.138114Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"{'translation': {'en': 'Rachel Pike : The science behind a climate headline',\n  'vi': 'Khoa học đằng sau một tiêu đề về khí hậu'},\n 'en_tokens': ['<sos>',\n  'rachel',\n  'pike',\n  'the',\n  'science',\n  'behind',\n  'a',\n  'climate',\n  'headline',\n  '<eos>'],\n 'vi_tokens': ['<sos>',\n  'khoa',\n  'học',\n  'đằng',\n  'sau',\n  'một',\n  'tiêu',\n  'đề',\n  'về',\n  'khí',\n  'hậu',\n  '<eos>'],\n 'en_ids': [2, 6429, 17576, 6, 295, 553, 11, 682, 5334, 3],\n 'vi_ids': [2, 300, 66, 1070, 109, 9, 360, 117, 37, 398, 700, 3]}"},"metadata":{}}]},{"cell_type":"code","source":"def collate_fn(batch):\n    source_batch = [torch.tensor(sample[SOURCE_LANG + \"_ids\"]) for sample in batch]\n    target_batch = [torch.tensor(sample[TARGET_LANG + \"_ids\"]) for sample in batch]\n\n    #source_batch = torch.tensor(source_batch)\n    #target_batch = torch.tensor(target_batch)\n    # Source batch will have size (batch_size, length of longest sequence)\n    # Same for target batch\n    source_batch = nn.utils.rnn.pad_sequence(source_batch, padding_value = PAD_IDX, batch_first = True)\n    target_batch = nn.utils.rnn.pad_sequence(target_batch, padding_value = PAD_IDX, batch_first = True)\n\n    return source_batch, target_batch","metadata":{"execution":{"iopub.status.busy":"2024-02-22T08:22:09.716085Z","iopub.execute_input":"2024-02-22T08:22:09.716431Z","iopub.status.idle":"2024-02-22T08:22:09.722574Z","shell.execute_reply.started":"2024-02-22T08:22:09.716406Z","shell.execute_reply":"2024-02-22T08:22:09.721627Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nBATCH_SIZE = 16\ntrain_data_loader = torch.utils.data.DataLoader(\n    dataset = train_data,\n    batch_size = BATCH_SIZE,\n    collate_fn = collate_fn,\n    shuffle = True\n)\n\ntest_data_loader = DataLoader(\n    dataset = test_data,\n    batch_size = BATCH_SIZE,\n    collate_fn = collate_fn,\n)\n\nvalid_data_loader = DataLoader(\n    dataset = valid_data,\n    batch_size = BATCH_SIZE,\n    collate_fn = collate_fn,\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-22T08:22:10.819068Z","iopub.execute_input":"2024-02-22T08:22:10.819782Z","iopub.status.idle":"2024-02-22T08:22:10.826216Z","shell.execute_reply.started":"2024-02-22T08:22:10.819749Z","shell.execute_reply":"2024-02-22T08:22:10.825305Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, attention_head, model_dimension):\n        super(MultiHeadAttention, self).__init__()\n        assert model_dimension % attention_head == 0, \"dimension of model must be divisible by the attention head\"\n\n        self.attention_head = attention_head\n        self.model_dimension = model_dimension\n        self.d_k = self.model_dimension // self.attention_head\n\n        # All of the below has shape\n        self.W_q = nn.Linear(model_dimension, model_dimension, bias = False) # Query transformation\n        self.W_k = nn.Linear(model_dimension, model_dimension, bias = False) # Key transformation\n        self.W_v = nn.Linear(model_dimension, model_dimension, bias = False) # Value transformation\n        self.W_o = nn.Linear(model_dimension, model_dimension) # Output transformation\n\n    def scaled_dot_products(self, Q, K, V, mask = None):\n        # Q has shape (batch_size, num_heads, n_q, d_k)\n        # K has shape (batch_size, num_heads, n_k, d_k)\n        # V has shape (batch_size, num_heads, n_v, d_k)\n        # Where n_q, n_k, n_v are seq_len of either src / tgt sequence\n        # Mask has shape (batch_size, 1, 1, src_seq_len)\n        # Mask has shape (batch_size, 1, tgt_seq_len, tgt_seq_len)\n        \n        # After tranposing, K has shape (batch_sze, num_heads, d_k, n_k)\n        attention_score = (Q @ K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        # Attention_score has shape (batch_size, num_heads, n_q, n_k)\n        \n        if mask is not None:\n            mask = mask.unsqueeze(1)\n            attention_score = attention_score.masked_fill(mask == 0, value = -1e9)\n\n        # Attention_probability is computed via softmax function\n        # Still have shape (batch_size, num_heads, n_q, n_k)\n        attention_probability = torch.softmax(attention_score, dim = -1)\n\n\n        # (batch_size, num_heads, n_q, n_k) * (batch_size, num_heads, n_v, d_k)\n        # In multi-head self-attention there are 2 cases:\n        # If it's not cross attention, then \n        # Q = X * W_q, K = X * W_k, V = X * W_v\n        # If it is then \n        # Q = X * W_q, K = encoder_output * W_k, V = encoder_output * W_k\n        # => output has shape (batch_size, num_heads, n_q, d_k) \n        output = (attention_probability @ V)\n        return output\n\n    def split_heads(self, X):\n        '''\n        Reshape input X to have attention_head for multi-head attention\n        '''\n\n        # Tensor X has shape (batch_size, seq_len, model_dimension)\n\n        batch_size, seq_len, model_dimension = X.shape\n        output = X.view(batch_size, seq_len, self.attention_head, self.d_k)\n        output = output.transpose(1, 2)\n        # Return Tensor has shape (batch_size, attention_head, seq_len, dimension of each head)\n        return output\n\n    def combine_heads(self, X):\n        '''\n        Reshape input tensor X to have the same dimension before being fed for\n        Multi head attention    \n        '''\n        # Tensor X has shape (batch_size, attention_head, seq_len, dim_each_head)\n        # X.tranpose(1, 2) has shape (batch_size, seq_len, attention_head, dim_each_head)\n        \n        batch_size, _, seq_len, d_k = X.shape\n        output = X.transpose(1, 2).contiguous().view(batch_size, seq_len, self.model_dimension)\n        return output\n\n    def forward(self, Q, K, V, mask = None):\n\n        # Seems like Q, K, V all has shape (batch_size, n, d)\n        # Where n is input sequence length\n        # Where d is embedding dimension / model_dimension\n        # To simplify this, I will consider model_dimension\n        # And embedding dimension the same\n        # But if embedding dimension is different then\n        # We can change input dimension of Wo, Wv, Wk, Wq\n\n        Q = self.split_heads(self.W_q(Q))\n        K = self.split_heads(self.W_k(K))\n        V = self.split_heads(self.W_v(V))\n\n        attention_output = self.scaled_dot_products(Q, K, V, mask)\n\n        # attention_output has shape (batch_size, num_heads, n_q, d_k)\n\n        output = self.W_o(self.combine_heads(attention_output))\n        # combine heads we have (batch_size, n_q, model_dimension)\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-02-22T08:30:00.385635Z","iopub.execute_input":"2024-02-22T08:30:00.386373Z","iopub.status.idle":"2024-02-22T08:30:00.400806Z","shell.execute_reply.started":"2024-02-22T08:30:00.386343Z","shell.execute_reply":"2024-02-22T08:30:00.399814Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"class PositionWiseFeedForwardNetwork(nn.Module):\n    def __init__(self, model_dimension, feed_forward_dimension):\n\n        super(PositionWiseFeedForwardNetwork, self).__init__()\n        self.model_dimension = model_dimension\n        self.feed_forward_dimension = feed_forward_dimension\n\n        self.fc1 = nn.Linear(model_dimension, feed_forward_dimension)\n        self.fc2 = nn.Linear(feed_forward_dimension, model_dimension)\n        self.relu = nn.ReLU()\n\n    def forward(self, X):\n        # X has shape (batch_size, seq_len, model_dimension)\n        # Return tensor has the same thing\n        return self.fc2(self.relu(self.fc1(X)))","metadata":{"execution":{"iopub.status.busy":"2024-02-22T08:30:00.751552Z","iopub.execute_input":"2024-02-22T08:30:00.751877Z","iopub.status.idle":"2024-02-22T08:30:00.758262Z","shell.execute_reply.started":"2024-02-22T08:30:00.751853Z","shell.execute_reply":"2024-02-22T08:30:00.757327Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, model_dimension, max_seq_len, dropout):\n        super(PositionalEncoding, self).__init__()\n        self.model_dimension = model_dimension\n        self.max_seq_len = max_seq_len\n        self.dropout = nn.Dropout(dropout)\n        positional_encoding = torch.zeros(max_seq_len, model_dimension)\n        position = torch.arange(0, max_seq_len, dtype = torch.float).unsqueeze(1)\n\n        # position has shape (max_seq_len, 1)\n        # positional_encoding has shape (max_seq_len, model_dimension)\n\n        # log(pos) - 2i/model * log(10000)\n        div_term = torch.exp(torch.arange(0, model_dimension, 2).float() * -(math.log(10000.0) / model_dimension))\n\n        positional_encoding[:, 0::2] = torch.sin(position * div_term)\n        positional_encoding[:, 1::2] = torch.cos(position * div_term)\n        positional_encoding = positional_encoding.unsqueeze(0)\n        \n        self.register_buffer('pe', positional_encoding)\n\n    def forward(self, X):\n        # X has shape (batch_size, seq_len, model_dimension)\n        return self.dropout(X + (self.pe[:, :X.shape[1], :]))","metadata":{"execution":{"iopub.status.busy":"2024-02-22T08:30:01.123490Z","iopub.execute_input":"2024-02-22T08:30:01.124216Z","iopub.status.idle":"2024-02-22T08:30:01.133122Z","shell.execute_reply.started":"2024-02-22T08:30:01.124176Z","shell.execute_reply":"2024-02-22T08:30:01.132145Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"class EncoderBlock(nn.Module):\n    def __init__(self, model_dimension, attention_heads, feed_forward_dimension, dropout):\n        super(EncoderBlock, self).__init__()\n\n        self.attention = MultiHeadAttention(attention_heads, model_dimension)\n        self.feed_forward_network = PositionWiseFeedForwardNetwork(model_dimension, feed_forward_dimension)\n        self.norm1 = nn.LayerNorm(model_dimension)\n        self.norm2 = nn.LayerNorm(model_dimension)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, X, mask):\n        # X is a tensor of shape (batch_size, seq_len, model_dimmension)\n        attention_output = self.attention(X, X, X, mask)\n        X = self.norm1(X + self.dropout(attention_output))\n        feed_forward_output = self.feed_forward_network(X)\n        X = self.norm2(X + self.dropout(feed_forward_output))\n\n        # Return tensor is the same size\n        return X\n","metadata":{"execution":{"iopub.status.busy":"2024-02-22T08:30:01.535342Z","iopub.execute_input":"2024-02-22T08:30:01.536084Z","iopub.status.idle":"2024-02-22T08:30:01.543224Z","shell.execute_reply.started":"2024-02-22T08:30:01.536051Z","shell.execute_reply":"2024-02-22T08:30:01.542262Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"class DecoderBlock(nn.Module):\n    def __init__(self, model_dimension, attention_heads, feed_forward_dimension, dropout):\n        super(DecoderBlock, self).__init__()\n\n        self.attention = MultiHeadAttention(attention_heads, model_dimension)\n        self.feed_forward_network = PositionWiseFeedForwardNetwork(model_dimension, feed_forward_dimension)\n        self.norm1 = nn.LayerNorm(model_dimension)\n        self.norm2 = nn.LayerNorm(model_dimension)\n        self.norm3 = nn.LayerNorm(model_dimension)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, X, encoder_output, source_mask, target_mask):\n\n        attention_output = self.attention(X, X, X, target_mask)\n        X = self.norm1(X + self.dropout(attention_output))\n        # X after add & norm layer has the same shape (batch_size, tgt_len, model_dim)    \n        # In this scenario, encoder output will be played as key and value\n        # This is cross-attention\n        # Encoder_output has shape (batch_size, src_len, model_dim)\n        attention_output = self.attention(X, encoder_output, encoder_output, source_mask)\n        \n        # Attention_output has shape (batch_size, tgt_len, model_dim)\n        X = self.norm2(X + self.dropout(attention_output))\n            \n        feed_forward_output = self.feed_forward_network(X)\n        X = self.norm3(X + self.dropout(feed_forward_output))\n\n        return X","metadata":{"execution":{"iopub.status.busy":"2024-02-22T08:30:01.913365Z","iopub.execute_input":"2024-02-22T08:30:01.913720Z","iopub.status.idle":"2024-02-22T08:30:01.922725Z","shell.execute_reply.started":"2024-02-22T08:30:01.913690Z","shell.execute_reply":"2024-02-22T08:30:01.921743Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, model_dimension, attention_heads, feed_forward_dimension,\n                 source_vocab_size, target_vocab_size, num_layers, max_seq_len, dropout):\n        super(Transformer, self).__init__()\n\n        self.model_dimension = model_dimension\n        self.attention_heads = attention_heads\n        self.feed_forward_dimension = feed_forward_dimension\n\n        self.positional_encoding = PositionalEncoding(model_dimension, max_seq_len, dropout)\n        self.encoder_embedding = nn.Embedding(source_vocab_size, model_dimension)\n        self.decoder_embedding = nn.Embedding(target_vocab_size, model_dimension)\n        self.encoders = nn.ModuleList([EncoderBlock(model_dimension, attention_heads, feed_forward_dimension, dropout) for _ in range(num_layers)])\n        self.decoders = nn.ModuleList([DecoderBlock(model_dimension, attention_heads, feed_forward_dimension, dropout) for _ in range(num_layers)])\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(model_dimension, target_vocab_size)\n    \n    \n    def generate_mask(self, source_sentence, target_sentence):\n        # target sentence has shape (batch_size, max_tgt_len)\n        # source sentence has shape (batch_size, max_src_len)\n        # since batch_size can be broadcasted so we can skip it\n\n        batch_size = source_sentence.shape[0]\n        max_target_len = target_sentence.shape[1]\n        #print((target_sentence != PAD_IDX).unsqueeze(1))\n        # source_sentence has shape (batch_size, 1, max_src_len)\n        # = 1 if it's not PAD_IDX, 0 otherwise\n        source_mask = (source_sentence != PAD_IDX).unsqueeze(1).int().to(device)\n        \n        # target_mask has shape (batch_size, 1, max_tgt_len)\n        target_mask = (target_sentence != PAD_IDX).unsqueeze(1).int().to(device)\n\n        # no peak mask has shape (batch_size, max_tgt_len, max_tgt_len)\n        # = 0 if i >= j, 1 otherwise\n        no_peak_mask = 1 - torch.triu(torch.ones((1, max_target_len, max_target_len)), diagonal=1).type(torch.int).to(device)\n\n        # every position (i,j) such that i >= j and not a PAD_IDX\n        target_mask = target_mask & no_peak_mask\n        # target_mask now has shape (batch_size, max_tgt_len, max_tgt_len)\n\n        return source_mask, target_mask\n\n    def forward(self, source_sentence, target_sentence):\n\n        # source sentence have shape (batch_size, src_seq_len)\n        # target sentence have shape (batch_size, tgt_seq_len)\n        source_mask, target_mask = self.generate_mask(source_sentence, target_sentence)\n        source_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(source_sentence)))\n        target_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(target_sentence)))\n\n        # Now source_embedded have shape (batch_size, src_seq_len, model_dimension)\n        # Now target_embedded have shape (batch_size, tgt_seq_len, model_dimension)\n        \n        encoder_output = source_embedded\n        for encoder_layer in self.encoders:\n            encoder_output = encoder_layer(encoder_output, source_mask)\n        # encoder_output has shape (batch_size, src_seq_len, model_dimension)\n        \n        decoder_output = target_embedded\n        for decoder_layer in self.decoders:\n            decoder_output = decoder_layer(decoder_output, encoder_output, source_mask, target_mask)\n\n        # Output will have shape (batch_size, max_seq_len, target_vocab_size)\n        output = self.fc(decoder_output)\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-02-22T08:30:02.350318Z","iopub.execute_input":"2024-02-22T08:30:02.351001Z","iopub.status.idle":"2024-02-22T08:30:02.364896Z","shell.execute_reply.started":"2024-02-22T08:30:02.350963Z","shell.execute_reply":"2024-02-22T08:30:02.363817Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"model_dimension = 512\nattention_heads = 8\nfeed_forward_dimension = 2048\nsource_vocab_size = len(vocab[SOURCE_LANG])\ntarget_vocab_size = len(vocab[TARGET_LANG])\nnum_layers = 6\nmax_seq_len = 1000\ndropout = 0.1\n\n\n\nmodel = Transformer(\n    model_dimension,\n    attention_heads,\n    feed_forward_dimension,\n    source_vocab_size,\n    target_vocab_size,\n    num_layers,\n    max_seq_len,\n    dropout,\n)\nmodel= nn.DataParallel(model)\nmodel.to(device)\n#model_dimension, attention_heads, feed_forward_dimension,\n#source_vocab_size, target_vocab_size, num_layers, max_seq_len, dropout","metadata":{"execution":{"iopub.status.busy":"2024-02-22T08:30:02.774696Z","iopub.execute_input":"2024-02-22T08:30:02.775049Z","iopub.status.idle":"2024-02-22T08:30:03.767486Z","shell.execute_reply.started":"2024-02-22T08:30:02.775020Z","shell.execute_reply":"2024-02-22T08:30:03.766368Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"DataParallel(\n  (module): Transformer(\n    (positional_encoding): PositionalEncoding(\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder_embedding): Embedding(47271, 512)\n    (decoder_embedding): Embedding(21114, 512)\n    (encoders): ModuleList(\n      (0-5): 6 x EncoderBlock(\n        (attention): MultiHeadAttention(\n          (W_q): Linear(in_features=512, out_features=512, bias=False)\n          (W_k): Linear(in_features=512, out_features=512, bias=False)\n          (W_v): Linear(in_features=512, out_features=512, bias=False)\n          (W_o): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (feed_forward_network): PositionWiseFeedForwardNetwork(\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (relu): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (decoders): ModuleList(\n      (0-5): 6 x DecoderBlock(\n        (attention): MultiHeadAttention(\n          (W_q): Linear(in_features=512, out_features=512, bias=False)\n          (W_k): Linear(in_features=512, out_features=512, bias=False)\n          (W_v): Linear(in_features=512, out_features=512, bias=False)\n          (W_o): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (feed_forward_network): PositionWiseFeedForwardNetwork(\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (relu): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (dropout): Dropout(p=0.1, inplace=False)\n    (fc): Linear(in_features=512, out_features=21114, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\nprint(f\"The model has {count_parameters(model):,} trainable parameters\")","metadata":{"execution":{"iopub.status.busy":"2024-02-22T08:30:10.422462Z","iopub.execute_input":"2024-02-22T08:30:10.422818Z","iopub.status.idle":"2024-02-22T08:30:10.430173Z","shell.execute_reply.started":"2024-02-22T08:30:10.422790Z","shell.execute_reply":"2024-02-22T08:30:10.429131Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"The model has 83,660,922 trainable parameters\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr = 1e-4)\ncriterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)","metadata":{"execution":{"iopub.status.busy":"2024-02-22T08:30:12.538878Z","iopub.execute_input":"2024-02-22T08:30:12.539372Z","iopub.status.idle":"2024-02-22T08:30:12.545467Z","shell.execute_reply.started":"2024-02-22T08:30:12.539335Z","shell.execute_reply":"2024-02-22T08:30:12.544422Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"def train_fn(model, data_loader, optimizer, criterion, clip, device):\n    model.train()\n    epoch_loss = 0\n    for i, batch in enumerate(data_loader):\n        \n        source_sentence = batch[0].to(device)\n        target_sentence = batch[1].to(device)\n        # source_sentence has shape (batch_size, max_src_len)\n        # target_sentence has shape (batch_size, max_tgt_len)\n        batch_size = source_sentence.shape[0]\n        target_len = target_sentence.shape[1]\n        target_input = target_sentence[:, :-1]\n        # Each word i put into transformer will predict word i + 1 \n        # So no need for last word \n        target_output = target_sentence[:, 1:]\n        # Same for above comment => dont need first word for output\n        optimizer.zero_grad()\n\n        # output has shape (batch_size, max_tgt_len - 1, tgt_vocab_size)\n        output = model(source_sentence, target_input)\n        output = output.reshape(batch_size * (target_len - 1), output.shape[-1])\n        target_output = target_output.reshape(batch_size * (target_len - 1))\n        loss = criterion(output, target_output)\n        \n        if i % (len(data_loader) // 10) == 0:\n            print(i // (len(data_loader) // 10), \"%\", end = ' ') \n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        epoch_loss += loss.item()\n        \n    print()\n    return epoch_loss / len(data_loader)","metadata":{"execution":{"iopub.status.busy":"2024-02-22T08:33:17.054689Z","iopub.execute_input":"2024-02-22T08:33:17.055088Z","iopub.status.idle":"2024-02-22T08:33:17.065438Z","shell.execute_reply.started":"2024-02-22T08:33:17.055057Z","shell.execute_reply":"2024-02-22T08:33:17.064466Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"def evaluate_fn(model, data_loader, criterion, device):\n    model.eval()\n    epoch_loss = 0\n    with torch.no_grad():\n        for i, batch in enumerate(data_loader):\n            source_sentence = batch[0].to(device)\n            target_sentence = batch[1].to(device)\n\n            # source_sentence has shape (batch_size, max_src_len)\n            # target_sentence has shape (batch_size, max_tgt_len)\n            \n            batch_size = source_sentence.shape[0]\n            target_len = target_sentence.shape[1]\n            target_input = target_sentence[:, :-1]\n            target_output = target_sentence[:, 1:]\n            \n            output = model(source_sentence, target_input)\n            output = output.reshape(batch_size * (target_len - 1), output.shape[-1])\n            target_output = target_output.reshape(batch_size * (target_len - 1))\n            \n            loss = criterion(output, target_output)\n            epoch_loss += loss.item()\n    return epoch_loss / len(data_loader)","metadata":{"execution":{"iopub.status.busy":"2024-02-22T08:33:17.892852Z","iopub.execute_input":"2024-02-22T08:33:17.893776Z","iopub.status.idle":"2024-02-22T08:33:17.901431Z","shell.execute_reply.started":"2024-02-22T08:33:17.893742Z","shell.execute_reply":"2024-02-22T08:33:17.900439Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"import tqdm\n\nn_epochs = 10\nclip = 1\n#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nbest_valid_loss = float(\"inf\")\n\nfor epoch in tqdm.tqdm(range(n_epochs)):\n    train_loss = train_fn(model, train_data_loader, optimizer, criterion, clip, device)\n    valid_loss = evaluate_fn(model, valid_data_loader, criterion, device)\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), filepath + \"model.pth\")\n    print(f\"\\tTrain Loss: {train_loss:7.3f} | Train PPL: {np.exp(train_loss):7.3f}\")\n    print(f\"\\tValid Loss: {valid_loss:7.3f} | Valid PPL: {np.exp(valid_loss):7.3f}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-22T08:33:19.356305Z","iopub.execute_input":"2024-02-22T08:33:19.357022Z","iopub.status.idle":"2024-02-22T12:28:09.950942Z","shell.execute_reply.started":"2024-02-22T08:33:19.356991Z","shell.execute_reply":"2024-02-22T12:28:09.949727Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stderr","text":"  0%|          | 0/10 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"0 % 1 % 2 % 3 % 4 % 5 % 6 % 7 % 8 % 9 % 10 % \n","output_type":"stream"},{"name":"stderr","text":" 10%|█         | 1/10 [23:29<3:31:29, 1409.97s/it]","output_type":"stream"},{"name":"stdout","text":"\tTrain Loss:   4.038 | Train PPL:  56.698\n\tValid Loss:   3.312 | Valid PPL:  27.435\n0 % 1 % 2 % 3 % 4 % 5 % 6 % 7 % 8 % 9 % 10 % \n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 2/10 [46:57<3:07:46, 1408.30s/it]","output_type":"stream"},{"name":"stdout","text":"\tTrain Loss:   3.015 | Train PPL:  20.396\n\tValid Loss:   2.816 | Valid PPL:  16.716\n0 % 1 % 2 % 3 % 4 % 5 % 6 % 7 % 8 % 9 % 10 % \n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 3/10 [1:10:34<2:44:45, 1412.23s/it]","output_type":"stream"},{"name":"stdout","text":"\tTrain Loss:   2.571 | Train PPL:  13.080\n\tValid Loss:   2.540 | Valid PPL:  12.684\n0 % 1 % 2 % 3 % 4 % 5 % 6 % 7 % 8 % 9 % 10 % \n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 4/10 [1:33:54<2:20:45, 1407.57s/it]","output_type":"stream"},{"name":"stdout","text":"\tTrain Loss:   2.294 | Train PPL:   9.917\n\tValid Loss:   2.374 | Valid PPL:  10.743\n0 % 1 % 2 % 3 % 4 % 5 % 6 % 7 % 8 % 9 % 10 % \n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 5/10 [1:57:29<1:57:30, 1410.13s/it]","output_type":"stream"},{"name":"stdout","text":"\tTrain Loss:   2.100 | Train PPL:   8.168\n\tValid Loss:   2.295 | Valid PPL:   9.927\n0 % 1 % 2 % 3 % 4 % 5 % 6 % 7 % 8 % 9 % 10 % \n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 6/10 [2:21:02<1:34:05, 1411.28s/it]","output_type":"stream"},{"name":"stdout","text":"\tTrain Loss:   1.954 | Train PPL:   7.055\n\tValid Loss:   2.233 | Valid PPL:   9.332\n0 % 1 % 2 % 3 % 4 % 5 % 6 % 7 % 8 % 9 % 10 % \n","output_type":"stream"},{"name":"stderr","text":" 70%|███████   | 7/10 [2:44:21<1:10:21, 1407.29s/it]","output_type":"stream"},{"name":"stdout","text":"\tTrain Loss:   1.836 | Train PPL:   6.274\n\tValid Loss:   2.216 | Valid PPL:   9.170\n0 % 1 % 2 % 3 % 4 % 5 % 6 % 7 % 8 % 9 % 10 % \n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 8/10 [3:07:57<46:59, 1409.94s/it]  ","output_type":"stream"},{"name":"stdout","text":"\tTrain Loss:   1.738 | Train PPL:   5.685\n\tValid Loss:   2.186 | Valid PPL:   8.900\n0 % 1 % 2 % 3 % 4 % 5 % 6 % 7 % 8 % 9 % 10 % \n","output_type":"stream"},{"name":"stderr","text":" 90%|█████████ | 9/10 [3:31:15<23:26, 1406.29s/it]","output_type":"stream"},{"name":"stdout","text":"\tTrain Loss:   1.653 | Train PPL:   5.222\n\tValid Loss:   2.177 | Valid PPL:   8.821\n0 % 1 % 2 % 3 % 4 % 5 % 6 % 7 % 8 % 9 % 10 % \n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [3:54:50<00:00, 1409.06s/it]","output_type":"stream"},{"name":"stdout","text":"\tTrain Loss:   1.577 | Train PPL:   4.840\n\tValid Loss:   2.175 | Valid PPL:   8.806\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"filepath = '/kaggle/working/'\ntorch.save(model.state_dict(), filepath + \"model.pt\")","metadata":{"execution":{"iopub.status.busy":"2024-02-22T12:28:42.467333Z","iopub.execute_input":"2024-02-22T12:28:42.467660Z","iopub.status.idle":"2024-02-22T12:28:43.111143Z","shell.execute_reply.started":"2024-02-22T12:28:42.467636Z","shell.execute_reply":"2024-02-22T12:28:43.110362Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"filepath = '/kaggle/working/'\nmodel.load_state_dict(torch.load(filepath + \"model.pt\", map_location = device))\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-02-22T12:29:39.797703Z","iopub.execute_input":"2024-02-22T12:29:39.798627Z","iopub.status.idle":"2024-02-22T12:29:40.095824Z","shell.execute_reply.started":"2024-02-22T12:29:39.798592Z","shell.execute_reply":"2024-02-22T12:29:40.094915Z"},"trusted":true},"execution_count":63,"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"DataParallel(\n  (module): Transformer(\n    (positional_encoding): PositionalEncoding(\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder_embedding): Embedding(47271, 512)\n    (decoder_embedding): Embedding(21114, 512)\n    (encoders): ModuleList(\n      (0-5): 6 x EncoderBlock(\n        (attention): MultiHeadAttention(\n          (W_q): Linear(in_features=512, out_features=512, bias=False)\n          (W_k): Linear(in_features=512, out_features=512, bias=False)\n          (W_v): Linear(in_features=512, out_features=512, bias=False)\n          (W_o): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (feed_forward_network): PositionWiseFeedForwardNetwork(\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (relu): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (decoders): ModuleList(\n      (0-5): 6 x DecoderBlock(\n        (attention): MultiHeadAttention(\n          (W_q): Linear(in_features=512, out_features=512, bias=False)\n          (W_k): Linear(in_features=512, out_features=512, bias=False)\n          (W_v): Linear(in_features=512, out_features=512, bias=False)\n          (W_o): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (feed_forward_network): PositionWiseFeedForwardNetwork(\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (relu): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (dropout): Dropout(p=0.1, inplace=False)\n    (fc): Linear(in_features=512, out_features=21114, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"test_loss = evaluate_fn(model, test_data_loader, criterion, device)\nprint(f\"\\tTest Loss: {test_loss:7.3f} | Train PPL: {np.exp(test_loss):7.3f}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-22T12:29:42.947700Z","iopub.execute_input":"2024-02-22T12:29:42.948083Z","iopub.status.idle":"2024-02-22T12:29:48.190966Z","shell.execute_reply.started":"2024-02-22T12:29:42.948053Z","shell.execute_reply":"2024-02-22T12:29:48.189965Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"\tTest Loss:   2.175 | Train PPL:   8.806\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_pad_mask(sentence, pad_idx):\n    mask = (sentence != pad_idx).unsqueeze(1).int().to(device)\n    return mask\n\ndef get_no_peak_mask(sentence):\n    sentence_len = sentence.shape[1]\n    no_peak_mask = 1 - torch.triu(torch.ones((1, sentence_len, sentence_len)), diagonal=1).type(torch.int).to(device)\n    return no_peak_mask","metadata":{"execution":{"iopub.status.busy":"2024-02-22T12:30:24.174518Z","iopub.execute_input":"2024-02-22T12:30:24.175341Z","iopub.status.idle":"2024-02-22T12:30:24.182758Z","shell.execute_reply.started":"2024-02-22T12:30:24.175300Z","shell.execute_reply":"2024-02-22T12:30:24.181657Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\n\ndef greedy_decode(model, sentence, max_len = 100):\n    model.eval()\n    \n    input_tokens = token_transform['en'](sentence)\n    input_ids = [SOS_IDX] + vocab['en'].lookup_indices(input_tokens) + [EOS_IDX]\n    input_tensor = torch.tensor(input_ids).unsqueeze(0).to(device)\n    input_mask = get_pad_mask(input_tensor, PAD_IDX)\n    with torch.no_grad():\n        input_embedding = model.module.encoder_embedding(input_tensor)\n        input_embedding = model.module.positional_encoding(input_embedding)\n        encoder_output = input_embedding\n        for encoder_layer in model.module.encoders:\n            encoder_output = encoder_layer(encoder_output, input_mask)\n            \n    output_ids = [SOS_IDX]\n    for i in range(max_len):\n        output_tensor = torch.tensor(output_ids).unsqueeze(0).to(device)\n        output_mask = get_pad_mask(output_tensor, PAD_IDX) & get_no_peak_mask(output_tensor)\n        with torch.no_grad():\n            output_embedding = model.module.decoder_embedding(output_tensor)\n            output_embedding = model.module.positional_encoding(output_embedding)\n            decoder_output = output_embedding\n            for decoder_layer in model.module.decoders:\n                decoder_output = decoder_layer(decoder_output, encoder_output, input_mask, output_mask)\n            output = model.module.fc(decoder_output)\n            \n        output = F.softmax(output, dim = -1) \n        output_id = output.argmax(dim = -1)[:, -1].item()\n        output_ids.append(output_id)\n        \n        if len(output_ids) > max_len or output_id == EOS_IDX:\n            break\n    \n    output_tokens = [vocab['vi'].get_itos()[idx] for idx in output_ids]\n    return output_tokens","metadata":{"execution":{"iopub.status.busy":"2024-02-22T14:28:47.780875Z","iopub.execute_input":"2024-02-22T14:28:47.781852Z","iopub.status.idle":"2024-02-22T14:28:47.793280Z","shell.execute_reply.started":"2024-02-22T14:28:47.781817Z","shell.execute_reply":"2024-02-22T14:28:47.792392Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"text = 'i am studying artificial intelligence'\nprint(' '.join(greedy_decode(model, text, 500)))","metadata":{"execution":{"iopub.status.busy":"2024-02-22T14:28:49.845002Z","iopub.execute_input":"2024-02-22T14:28:49.845342Z","iopub.status.idle":"2024-02-22T14:28:49.988886Z","shell.execute_reply.started":"2024-02-22T14:28:49.845315Z","shell.execute_reply":"2024-02-22T14:28:49.987954Z"},"trusted":true},"execution_count":128,"outputs":[{"name":"stdout","text":"<sos> tôi đang nghiên cứu về trí thông minh nhân tạo <eos>\n","output_type":"stream"}]},{"cell_type":"code","source":"translations = [greedy_decode(model, example['translation']['en'].lower()) for example in test_data]","metadata":{"execution":{"iopub.status.busy":"2024-02-22T14:28:56.652958Z","iopub.execute_input":"2024-02-22T14:28:56.653324Z","iopub.status.idle":"2024-02-22T14:34:32.420366Z","shell.execute_reply.started":"2024-02-22T14:28:56.653293Z","shell.execute_reply":"2024-02-22T14:34:32.419516Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"references = [example['vi_tokens'] for example in test_data]","metadata":{"execution":{"iopub.status.busy":"2024-02-22T14:35:47.035979Z","iopub.execute_input":"2024-02-22T14:35:47.036721Z","iopub.status.idle":"2024-02-22T14:35:47.339375Z","shell.execute_reply.started":"2024-02-22T14:35:47.036692Z","shell.execute_reply":"2024-02-22T14:35:47.338448Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"code","source":"print(references[0])\nprint(translations[0])","metadata":{"execution":{"iopub.status.busy":"2024-02-22T14:35:49.047370Z","iopub.execute_input":"2024-02-22T14:35:49.048200Z","iopub.status.idle":"2024-02-22T14:35:49.052715Z","shell.execute_reply.started":"2024-02-22T14:35:49.048166Z","shell.execute_reply":"2024-02-22T14:35:49.051699Z"},"trusted":true},"execution_count":132,"outputs":[{"name":"stdout","text":"['<sos>', 'khi', 'tôi', 'còn', 'nhỏ', ',', 'tôi', 'nghĩ', 'rằng', 'bắctriều', 'tiên', 'là', 'đất', 'nước', 'tốt', 'nhất', 'trên', 'thế', 'giới', 'và', 'tôi', 'thường', 'hát', 'bài', '&quot', 'chúng', 'ta', 'chẳng', 'có', 'gì', 'phải', 'ghen', 'tị', '.', '&quot', '<eos>']\n['<sos>', 'khi', 'tôi', 'còn', 'nhỏ', ',', 'tôi', 'nghĩ', 'đất', 'nước', 'tôi', 'là', 'người', 'tốt', 'nhất', 'trên', 'hành', 'tinh', 'này', ',', 'và', 'tôi', 'lớn', 'lên', 'hát', 'một', 'bài', 'hát', 'có', 'tên', '&quot', 'không', 'gì', 'để', 'ghen', 'tị', '.', '&quot', '<eos>']\n","output_type":"stream"}]},{"cell_type":"code","source":"predictions = [example[1:-1] for example in translations]\nreferences = [example[1: -1] for example in references]","metadata":{"execution":{"iopub.status.busy":"2024-02-22T14:35:58.298688Z","iopub.execute_input":"2024-02-22T14:35:58.299429Z","iopub.status.idle":"2024-02-22T14:35:58.312059Z","shell.execute_reply.started":"2024-02-22T14:35:58.299397Z","shell.execute_reply":"2024-02-22T14:35:58.311092Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"references = [[example] for example in references]\nprint(predictions[900])\nprint(references[900])","metadata":{"execution":{"iopub.status.busy":"2024-02-22T14:36:20.885628Z","iopub.execute_input":"2024-02-22T14:36:20.886376Z","iopub.status.idle":"2024-02-22T14:36:20.892048Z","shell.execute_reply.started":"2024-02-22T14:36:20.886341Z","shell.execute_reply":"2024-02-22T14:36:20.891024Z"},"trusted":true},"execution_count":135,"outputs":[{"name":"stdout","text":"['nhưng', 'sư', 'tử', 'rất', 'thông', 'minh', '.']\n[['nhưng', 'sư', 'tử', 'rất', 'thông', 'minh', '.']]\n","output_type":"stream"}]},{"cell_type":"code","source":"from torchtext.data.metrics import bleu_score\n\nscore = bleu_score(predictions, references)\nprint(score)","metadata":{"execution":{"iopub.status.busy":"2024-02-22T14:36:25.678980Z","iopub.execute_input":"2024-02-22T14:36:25.679386Z","iopub.status.idle":"2024-02-22T14:36:26.529718Z","shell.execute_reply.started":"2024-02-22T14:36:25.679354Z","shell.execute_reply":"2024-02-22T14:36:26.528749Z"},"trusted":true},"execution_count":136,"outputs":[{"name":"stdout","text":"0.2640448933379371\n","output_type":"stream"}]}]}